{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0424c5c0",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b45f0",
   "metadata": {},
   "source": [
    "Ans: Overfitting:\n",
    "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "  \n",
    "When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data. If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.\n",
    "\n",
    "    \n",
    "    \n",
    "Underfitting:\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data.\n",
    "\n",
    " when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks.\n",
    "    \n",
    "An underfit model results in high prediction errors for both training and test data. An overfit model gives a very low prediction error on training data, but a very high prediction error on test data. Both types of models result in poor accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39f375",
   "metadata": {},
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e16a30",
   "metadata": {},
   "source": [
    "Ans: Ways to prevent overfitting include cross-validation, in which the data being used for training the model is chopped into folds or partitions and the model is run for each fold. Then, the overall error estimate is averaged. Other methods include ensembling: predictions are combined from at least two separate models, data augmentation, in which the available data set is made to look diverse, and data simplification, in which the model is streamlined to avoid overfitting.\n",
    "\n",
    "There are several techniques to avoid overfitting in Machine Learning altogether listed below.\n",
    "\n",
    "1. Cross-Validation: One of the most powerful features to avoid/prevent overfitting is cross-validation. The idea behind this is to use the initial training data to generate mini train-test-splits, and then use these splits to tune your model.\n",
    "\n",
    "2. Training With More Data: This technique might not work every time, as we have also discussed in the example above, where training with a significant amount of population helps the model. It basically helps the model in identifying the signal better.\n",
    "\n",
    "3. Removing Features: Although some algorithms have an automatic selection of features. For a significant number of those who do not have a built-in feature selection, we can manually remove a few irrelevant features from the input features to improve the generalization.\n",
    "\n",
    "4. Early Stopping: When the model is training, you can actually measure how well the model performs based on each iteration. We can do this until a point when the iterations improve the model’s performance. After this, the model overfits the training data as the generalization weakens after each iteration.\n",
    "\n",
    "5. Regularization: It basically means, artificially forcing your model to be simpler by using a broader range of techniques. It totally depends on the type of learner that we are using. For example, we can prune a decision tree, use a dropout on a neural network or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "6. Ensembling: This technique basically combines predictions from different Machine Learning models. Two of the most common methods for ensembling are listed below:\n",
    "\n",
    "        1.Bagging attempts to reduce the chance overfitting the models\n",
    "\n",
    "        2. Boosting attempts to improve the predictive flexibility of simpler models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338e607",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554b0af",
   "metadata": {},
   "source": [
    "Ans: Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.overfitting, happens when a machine learning model is not complex enough to accurately capture relationships between a dataset's features and a target variable.\n",
    "\n",
    "Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.\n",
    "\n",
    "It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60771ee0",
   "metadata": {},
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7adad6",
   "metadata": {},
   "source": [
    "Bias:Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "   \n",
    "Variance: Variance is the very opposite of Bias. During training, it allows our model to ‘see’ the data a certain number of times to find patterns in it. If it does not work on the data for long enough, it will not find patterns and bias occurs. On the other hand, if our model is allowed to view the data too many times, it will learn very well for only that data. It will capture most patterns in the data,  but it will also learn from the unnecessary data present, or from the noise.\n",
    "    \n",
    "Bias-Variance Tradeoff: \n",
    "Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff.\n",
    "\n",
    "\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\n",
    "\n",
    "The model will fit with the data set while increasing the chances of inaccurate predictions. The same applies when creating a low variance model with a higher bias. While it will reduce the risk of inaccurate predictions, the model will not properly match the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cdbdb1",
   "metadata": {},
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17385607",
   "metadata": {},
   "source": [
    "Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting.\n",
    "Some of the methods used to prevent overfitting include ensembling, data augmentation, data simplification, and cross-validation.\n",
    "\n",
    "\n",
    "Underfitting Machine Learning in general, we can't know how well our model will perform on new data until we put it to the test.\n",
    "\n",
    "To address this, we may divide our entire dataset into two subsets: a training subset and a test subset. This method can give us an indication of how well our model will perform on new data. (For population and sample size, go back and check how we did this to detect underfitting.)\n",
    "\n",
    "If our model does considerably better on the training set than the test set, we may be overfitting.\n",
    "The best strategy is to increase the model complexity by either increasing the number of parameters of your deep learning model or the order of your model. Underfitting is due to the model being simpler than needed. It fails to capture the patterns in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcd0bc",
   "metadata": {},
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757af2c3",
   "metadata": {},
   "source": [
    "Ans: Difference between Bias and Variance:\n",
    "        \n",
    "Bias: Bias is the difference between the average prediction and the correct value. It is also known as Bias Error or Error due to Bias.\n",
    "\n",
    "Low Bias models: k-Nearest Neighbors (k=1), Decision Trees and Support Vector Machines.\n",
    "High Bias models: Linear Regression and Logistic Regression.\n",
    "    \n",
    "    \n",
    "Variance: Variance is the amount that the prediction will change if different training data sets were used. It measures how scattered (inconsistent) are the predicted values from the correct value due to different training data sets.\n",
    "It is also known as Variance Error or Error due to Variance.\n",
    "\n",
    "Low Variance models: Linear Regression and Logistic Regression.\n",
    "High Variance models: k-Nearest Neighbors (k=1), Decision Trees and Support Vector Machines.\n",
    "    \n",
    "    \n",
    "High Bias - High Variance: Predictions are inconsistent and inaccurate on average.\n",
    "    \n",
    "  building a linear regression model over non-linear data. is the example of highbias and highvariance.  \n",
    "\n",
    "\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af5854",
   "metadata": {},
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce503b3",
   "metadata": {},
   "source": [
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "Regularization is a better technique than Reducing the number of features to overcome the overfitting problem as in Regularization we do not discard the features of the model.\n",
    "\n",
    "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase. And Linear regression model will try to optimize the coefficient in order to minimize the cost function.\n",
    "\n",
    "\n",
    "There are three main regularization techniques, namely:\n",
    "Ridge Regression (L2 Norm),Lasso (L1 Norm),Dropout.\n",
    "\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it.\n",
    "\n",
    "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights. The L1 regularization solution is sparse. The L2 regularization solution is non-sparse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
